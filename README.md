## Data-Analytics-Using-Apache-Spark

### Steps : 

1. Collected articles of various categories from NY Times using API and imported in Spark.
2. Built a data analytics pipeline for cleaning the collected data and extracting features.
3. Calculated TF-IDF values to extract features from tokenized and cleaned data.
4. Built a model for classification using different algorithms - Logistic Regression, Naive Bayes and Random Forest.
5. Classified the articles based on the category to which they belonged.
6. Assessed the accuracy using a query text.
7. Compared the classification accuracy of each algorithm for the given text data set.
